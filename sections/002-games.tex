\section{State-Separable Proofs}

Our framework for describing protocols is based on 
\emph{state-separable proofs}
\cite{AC:BDFKK18}.
The security notions we develop for protocols ultimately
find meaning in analogous notions of security for \emph{packages},
the main object of study in state-separable proofs.

This section is intended to be a suitable independent presentation
of this formalism.
In that spirit, we develop state-separable proofs ``from scratch''.
Our starting point is merely that of computable randomized functions.
This is in contrast to other protocol security frameworks like UC,
whose foundational starting point is usually the more concrete notion
of \emph{interactive Turing machines}.

We also take the opportunity to solidify the formalism of state-separable
proofs, providing more complete definitions of various objects,
completing several proofs left as mere sketches in the original paper,
and proving a few additional properties we'll need later.
This makes this section of interest to readers who are already familiar
with state-separable proofs.

\subsection{Some Notational Conventions}

We write $[n]$ to denote the set $\{1, \ldots, n\}$.

We write $\txt{01}$ to denote the set $\{0, 1\}$,
and $\str$ to denote binary strings.
We write $\bullet$ to denote the empty string,
which also serves as a ``dummy'' value in various contexts.

By $x \mapsto f(x)$, we mean a function taking in an input
$x$, and returning the value $f(x)$.
Sometimes we'll need to extend this syntax to more complicated expressions,
writing:
$$
x \mapsto y \gets f(x);\ g(x, y)
$$
to mean a function taking in an input value $x$,
then calling $f$ to produce a value $y$, before then using
both $x$ and $y$ to return the value $g(x, y)$.

\subsection{Probabilistic Functions}

Our starting point is the notion of \emph{randomized computable functions}.
This is a notion we assume can be defined in a rigorous way, but whose
concrete semantics we don't assign.
We write $f : \randf{\str}{\str}$ to denote such a function (named $f$).
Intuitively, this represents a function described by some algorithm,
which takes in a binary string as an input, and produces a binary string
as output, and is allowed to make randomized decisions to aid its computation.

We mainly consider \emph{families} of functions,
parameterized by a security parameter $\lambda$.
Formally, this is in fact a function $f : \mathbb{N} \to \randf{\str}{\str}$,
and we write $f_{\lambda} : \randf{\str}{\str}$ to denote a particular
function in the family.
In most cases, this security parameter is left \emph{implicit}.
In fact, all of the objects we consider from here on out will \emph{implicitly}
be \emph{families} of objects, parameterized by a security parameter $\lambda$,
but we will invoke this fact only as necessary.

\begin{definition}[Efficient Functions]
    We assume that a function family $f$ has a runtime, denoted $T(f, x)$,
    measuring how much time each function takes to execute on a family of inputs $x : \mathbb{N} \to \str$,
    as a function of $\lambda$.

    We say that a function family is \emph{efficient} if:
    $$
    \forall x, |x| \in \mathcal{O}(\tx{poly}(\lambda)).\quad T(f, x) \in \mathcal{O}(\tx{poly}(\lambda))
    $$
    In other words, the runtime is always polynomial in $\lambda$, regardless
    of its random choices, or its input (as long as that input is of a reasonable size).

    $\square$
\end{definition}

Functions which are not necessarily efficient are said to be \emph{unbounded}.

Considering efficient functions is essential for game-based security, because the vast majority
of cryptographic techniques depend on assuming that some problems are ``hard''
for adversaries with bounded computational resources.
Ironically, for protocol security, many protocols can be proven secure
without this restriction.

Another crucial notion we need to develop is that of a \emph{distance},
measuring how different two functions behave.
This will underpin our later notion of security for games,
which is based on saying that two different games are difficult
to tell apart.

\begin{definition}[Distance Function]
    Given a function $f : \randf{\bullet}{\bin}$, we let
    $P[f \to 1]$ denote the probability
    of the function returning $1$ on the input $\bullet$.

    Given two functions $f, g$, we define their distance $\varepsilon(f, g)$ as:
    $$
    \varepsilon(f, g) := |P[f \to 1] - P[g \to 1]|
    $$

    $\square$
\end{definition}

In other words, the distance looks at how often one function returns $1$
compared to the other.
If the functions agree most of the time, then their distance will be small,
whereas if they disagree very often, their distance will be large.
This definition is actually quite natural.
Since $P[f \to 1] = (1 - P[f \to 0])$, $\varepsilon$ is actually
just the total variation---or statistical---distance.
This immediately implies that this distance has some nice properties,
in particular that it forms a \emph{metric}.

\begin{lemma}[Distance is a Metric]
    $\varepsilon$ is a valid metric, in particular, it holds
    for any functions $f, g, h$, that:
    \begin{enumerate}
        \item $\varepsilon(f, f) = 0$,
        \item $\varepsilon(f, g) = \varepsilon(g, f)$,
        \item $\varepsilon(f, h) \leq \varepsilon(f, g) + \varepsilon(g, h)$.
    \end{enumerate}

    \txbf{Proof:}

    \txbf{1.} Follows from the fact that $P[f \to 1] = P[f \to 1]$,
    so $\varepsilon(f, f) = 0$.

    \txbf{2.} Follows from the fact that $|a - b| = |b - a|$.

    \txbf{3.} Follows from the triangle inequality for $\mathbb{R}$
    and the fact that:
    $$
    |P[f \to 1] - P[h \to 1]| = |(P[f \to 1] - P[g \to 1]) + (P[g \to 1] - P[h \to 1])|
    $$

    $\blacksquare$
\end{lemma}

Another property not included in our proof that $\varepsilon$ is a valid metric
requires that if $f \neq g$, then $\varepsilon(f, g) > 0$.
We omitted this property, because we haven't yet defined what $=$
should mean for functions.
Since we'd like this property to hold, we can simply define
equality in such a way that it does.

\begin{definition}[Function Equality]
    Two functions, $f$ and $g$, are \emph{equal}, written $f = g$, when:
    $$
    \varepsilon(f, g) = 0
    $$

    $\square$
\end{definition}

It's easy to see that this is an equality relation, satisfying
reflexivity, symmetry, and transitivity.

We can also generalize this to arbitrary functions, rather than just
$f : \randf{\bullet}{\bin}$, by defining:
$$
\varepsilon(f, g) := \tx{sup}_{x, y \in \str} |P[f(x) \to y] - P[g(x) \to y]|
$$
In other words, we look at the maximum difference
across all possible inputs and outputs.

However, we will not really be needing this general definition, outside
of a technical and very strong notion of equality for packages
used in the following subsection.

While the functions we've considered so far only manipulate binary strings,
it's useful to allow \emph{typed} functions,
with richer input and output types.
This could be defined in several ways, but the end result means
that a typed function $f : \randf{A}{B}$ can be interpreted as a function
over binary strings, using a suitable encoding and decoding mechanism,
as well as perhaps having a special output value that $f$ can return
if it fails to decode its input successfully.

Being able to quantify types is also useful for the formalism itself,
and potentially even for some packages.
As an example, consider the function $\tx{id}(x)$ which immediately
returns $x$.
This function is valid regardless of what type $x$ has.
Because of this, we might write this function formally as:
$$
\begin{aligned}
    &\tx{id} : \forall s.\ s \to s\cr
    &\tx{id} = x \mapsto x
\end{aligned}
$$
assigning it the type $\forall s.\ s \to s$.
In this type, $s$ is a quantified type variable, as indicated by the $\forall s$.
Formally, we can see $\tx{id}$ as a function parameterized by a type,
with $\tx{id}_S$ being a concrete function, after having chosen this type.

\subsection{Defining Packages}

Our next goal is to define the central object of state-separable proofs:
the \emph{package}.
Intuitively, a package has some kind of state, as well as functions
which manipulate this state.
You can interact with a package by calling the various output functions
it provides.
This makes packages a natural fit for security games.
What distinguishes packages from games is that they can have \emph{input}
functions.
A package can depend on another package, with each of its functions
potentially using the functions provided by this other package.
This modularity makes the common proof technique of ``game-hopping''
much more easily usable, and is the core strength of the state-separable
proof formalism.

Before we get to packages, we first need to define a few convenient
notions for functions manipulating a state, and parameterizing
functions with other functions.

Our first definition will be a little bit of shorthand.
\begin{definition}[Stateful Function]
    A \emph{stateful} function is simply a function $f$ of the form:
    $$
    f : \randf{(S, \str)}{(S, \str)}
    $$
    $S$ represents the state being used and modified by the function.
    As a convenient shorthand, we write:
    $$
    f :\ \stateful{S}
    $$

    $\square$
\end{definition}
It's useful to have a bit of typing to separate the state from the rest
of the input and output, since it allows us to avoid defining
inessential padding details inside the formalism itself.

We'll also want a notion of equality for these functions.

\begin{definition}[Stateful Function Equality]
    Two stateful functions $f : \stateful{S}$ and $f : \stateful{S'}$
    are equal, written $f = f'$, if there exists an isomorphism
    $\varphi : S \cong S'$, such that:
    $$
    f = (s, i) \mapsto (s', o) \gets f'(\varphi(s), i);\ (\varphi^{-1}(s'), o)
    $$

    $\square$
\end{definition}

Basically, the states don't have to be literally the same, as long
as they're isomorphic, and the natural way of making the two types
match up produces equal functions.
One can verify that this forms a valid equality relation.
Note that this reduces to the standard notion of equality of functions
by considering appropriate binary encodings of the two states.

We also need to consider functions parameterized by other functions.
Intuitively, this arises when one function calls another.
For example, consider:
$$
f(x) := g(x) \oplus g(x)
$$
which is well defined regardless of what $g$ is.
Here $f$ is implicitly parameterized by $g$, but we could write this explicitly
as $f(x) := g \mapsto g(x) \oplus g(x)$.
We could write $f : (\randf{\str}{\str}) \to (\randf{\str}{\str})$  
as a potential type in this example.
We write $f[g]$ for the instantiation of a parameterized function $f$
with an input function $g$.
It might also be the case that $g$ is itself parameterized,
in which case $f[g]$ is defined as:
$$
f[g] := h \mapsto f[g[h]]
$$

We can define a natural, albeit very strong, notion of equality for parameterized
functions, saying that:
$$
f = g \iff \forall h_1, \ldots, h_n.\ f[h_1, \ldots] = g[h_1, \ldots]
$$
In other words, the two functions must be equal regardless of how we instantiate
them.

We've now developed enough tools to define packages.

\begin{definition}[Package]
    A package $A$ consists of:
    \begin{itemize}
        \item a type $S$, for its state,
        \item a set of \emph{input names} $\tx{In}(A)$, of size $m$,
        \item a permutation $\pi_{\tx{in}} : \tx{In}(A) \leftrightarrow [m]$,
        \item a set of \emph{output names} $\tx{Out}(A)$, of size $n$,
        \item a permutation $\pi_{\tx{out}} : [n] \leftrightarrow \tx{Out}(A)$,
        \item a set of parameterized functions $f_1, \ldots, f_n : \forall s. \stateful{s}^m \to \stateful{(S, s)}$,
        each of which has a distinct name $n_i \in \tx{Out}(A)$.
    \end{itemize}

    We also only consider a package to be defined \emph{up to} potentially
    renaming its input and output functions injectively.

    $\blacksquare$
\end{definition}

Note that here $\stateful{s}^m$ denotes a tuple type containing
m values of type $\stateful{s}$.

We'll often use $\tx{In}(A)$ or $\tx{Out}(A)$ to talk about the input
and output functions of a package.
As a bit of a short hand notation, we write $\tx{In}(A, B, \ldots)$
for the union $\tx{In}(A) \cup \tx{In}(B) \cup \ldots$, and similarly
for $\tx{Out}(\ldots)$.

The motivation behind this definition is that a package has an internal state $S$, which gets
manipulated by each of the functions it exports.
These functions, in turn, can depend on other input functions.
If a stateful function $f : \forall s. \stateful{s} \to \stateful{(S_1, s)}$ uses a stateful function $g : \stateful{S_2}$,
then the result is a stateful function $f[g] : \stateful{(S_1, S_2)}$ manipulating 
\emph{both} the state of $f$, and the state of $g$.
Furthermore, $f$ is defined in such way agnostic to what the state manipulated
by $g$ happens to be,
which is why we use a \emph{quantified} type instead: to allow instantiation
with functions manipulating different kinds of state.
If $g$ used a state type $S'_2$, then $f[g]$ would have type $\stateful{(S_1, S'_2)}$
instead.

In practice, each function in a package is unlikely to use \emph{all}
of the input functions of the package, but it is much simpler
to have each function parameterized by all the possible inputs,
even if some are left unused.
It's also much simpler to define an ordering of the input functions
$\pi$, so that we can use $\stateful{s}^m$ as the input type for
the parameterized functions.

The semantics of a package without inputs are intuitively that of a stateful computer program
or machine you can interact with.
The machine has some kind of state, represented by $S$,
along with various functions you can call, represented by $f_1, \ldots, f_n$.
Each of these will use the input you provide, along with the current state
of the machine, in order to supply you with an output,
potentially modifying the state along the way.
The input functions allow a package to interact with other packages itself.


We describe this kind of interaction using the formal
notion of package \emph{composition}.

\begin{definition}[Package Composition]
    Given two packages $A, B$ with $\tx{In}(A) \subseteq \tx{Out}(B)$,
    we define their composition $A \circ B$ as a package characterized by:

    \begin{itemize}
        \item a state type $(A.S, B.S)$,
        \item input names $\tx{In}(B)$,
        \item output names $\tx{Out}(A)$,
        \item $\pi_{\tx{in}} := B.\pi_{\tx{in}}$,
        \item $\pi_{\tx{out}} := A.\pi_{\tx{out}}$,
        \item output functions $A.f_1[\varphi(B.f_1), \ldots, \varphi(B.f_{B.n})], \ldots$.
    \end{itemize}

    In more detail, these functions have type $\forall s.\stateful{s}^{B.m} \to \stateful{((A.S, B.S), s)}$,
    and are of the form:
    $$
    (h_1, \ldots, h_{B.m}) \mapsto A.f_i[\varphi_{A}(B.f_1)[h_1, \ldots], \ldots, \varphi_{A}(B.f_{B.n})[h_1, \ldots]]
    $$
    where $\varphi_{A}$ assigns each function $B.f_i$ to a slot in $[m]$
    using $A.\pi_{\tx{in}}$ on the name of that function, $B.n_i$.
    The same input functions $h_j$ are given
    to all the functions used by $A.f_i$.

    $\square$
\end{definition}

Package composition formally defines the intuitive notion of one package
``using'' the functions provided by another package.
The result is a package providing the functions defined in $A$,
and requiring the functions needed by $B$, but with the functions
inside $B$ itself now effectively inlined inside of $A \circ B$.

Next we'd like to prove that package composition satisfies some nice
properties.
For example $A \circ (B \circ C)$ is the same as $(A \circ B) \circ C$.
Before we can prove such properties, we need to define
what it means for two packages to be ``the same''.

\begin{definition}[Literal Equality]
    We say that two packages $A, B$ are \emph{literally equal},
    written $A \equiv B$, when:
    \begin{itemize}
        \item $A.S \cong B.S$,
        \item $\tx{In}(A) = \tx{In}(B)$,
        \item $\tx{Out}(A) = \tx{Out}(B)$,
        \item There exists a permutation $\pi : [n] \leftrightarrow [n]$ such that
        $$\forall i \in [n].\ A.f_i = B.f_{\pi(i)} \land A.n_i = B.n_{\pi(i)}$$
    \end{itemize}

    $\square$
\end{definition}

We require strict equality for the input and output names,
to avoid spurious comparisons between two packages with completely different names,
although it should be noted that packages are only really defined up to renaming anyways,
so this is essentially an isomorphism constraint.
For the type of state, we consider an isomorphism directly,
mainly so that $(A.S, (B.S, C.S))$ is considered to be the same state
type as $((A.S, B.S), C.S)$, which might already be the case depending on
how one defines equality for sets.
The final condition also implies that $\pi_{\tx{in}}$ is the same
for both packages.

This notion of equality is very strong, especially because of the equality
it imposes on the functions defined in each package.
While it suffices to explore basic properties of composition for packages,
we'll want to abandon it quite quickly for a looser and more easily
used notion of equality.

The first property we prove using this new definition is the one used as an example before.

\begin{lemma}[Associativity of Composition]
    Given packages $A, B, C$, it holds that:
    $$
    A \circ (B \circ C) \equiv (A \circ B) \circ C
    $$
    provided these expressions are well defined.

    \txbf{Proof:} The input and output names are clearly equal on both sides.
    Furthermore, the state on the left is $(A.S, (B.S, C.S))$,
    and $((A.S, B.S), C.S)$ on the right, and so the two states are isomorphic.
    All that's left is the final condition, talking about the equality
    of the functions defined in each package.

    Now, for the equality of functions, we'll expand the functions
    of the package on the left, and then on the right, before comparing
    the results we get.

    The functions in $B \circ C$ are of the form:
    $$
    (h_1, \ldots) \mapsto B.f_i[\varphi_{B}(C.f_1)[h_1, \ldots], \ldots]
    $$
    And then the functions in $A \circ (B \circ C)$ are of the form:
    $$
    (h_1, \ldots) \mapsto A.f_i[\varphi_{A}(B.f_1)[\varphi_{B}(C.f_1)[h_1, \ldots]], \ldots]
    $$

    From the other side, the functions in $A \circ B$ are of the form:
    $$
    (h_1, \ldots) \mapsto A.f_i[\varphi_{A}(B.f_1)[h_1, \ldots], \ldots]
    $$
    This makes the functions in $(A \circ B) \circ C$ of the form:
    $$
    (h_1, \ldots) \mapsto A.f_i[\varphi_{A}(B.f_1)[\varphi_{A \circ B}(C.f_1)[h_1, \ldots]], \ldots]
    $$

    The main difference is that we end up with $\varphi_{A \circ B}$
    as our means of assigning the functions in $C$ to the slots
    of $B$.
    However, $\varphi_{X}$ only depends on $X.\pi_{\tx{in}}$,
    and by definition $(A \circ B).\pi_{\tx{in}} = B.\pi_{\tx{in}}$,
    so $\varphi_{A \circ B} = \varphi_B$.

    Another smaller difference is that the resulting stateful functions
    have different, but isomorphic states, which is allowed
    by stateful function equality.

    So, in both cases, we end up with the same functions, concluding
    our proof.

    $\blacksquare$
\end{lemma}

This property is useful, since it lets us simply write $A \circ B \circ C$,
without worrying about the order in which packages are composed.

Another more technical property we want composition to satisfy is
that of \emph{equality preservation}.
If $B \equiv B'$, then it should be the case that $A \circ B \equiv A \circ B'$,
or that $B \circ C \equiv B' \circ C$.
If that weren't the case, then that would indicate that something is wrong
with our definition of either equality or composition.
The property we want for literal equality is that $A$ and $A'$ are completely
interchangeable, and so one can always be replaced with the other, no matter
the context, to the point that we can think of them as literally being the same
package.

Thankfully, it turns out that composition and literal equality do
in fact get along.

\begin{lemma}[Composition Preserves Equality]
    Given any packages $A, B, B', C$ it holds that:
    \begin{itemize}
        \item $B \equiv B' \implies A \circ B \equiv A \circ B'$,
        \item $B \equiv B' \implies B \circ C \equiv B' \circ C$,
    \end{itemize}
    provided these expressions are well defined.

    \txbf{Proof:} In one case the state type is $(A.S, B.S)$
    or $(A.S, B'.S)$, which are isomorphic if $B.S \cong B'.S$.
    Similarly, in the other case, we have $(B.S, C.S)$ vs $(B'.S, C.S)$,
    and the same observation holds.

    Now, remember that $\tx{In}(X \circ Y) = \tx{In}(Y)$, and $\tx{Out}(X \circ Y) = \tx{Out}(X)$.
    Thus, since both $\tx{In}(B) = \tx{In}(B')$ and $\tx{Out}(B) = \tx{Out}(B')$
    hold, we conclude that $\tx{In}$ and $\tx{Out}$ match up in both cases.

    The trickier part is the 4th condition for equality.

    In the first case, the functions are of the form:
    $$
    A.\tx{f}_i[\varphi_A(B.f_1), \ldots]
    $$
    Now, $\varphi_A$ orders the functions in $B$ based only on their \emph{names}.
    In particular, the ordering does not matter.
    Since the functions in $B'$ are the same as $B$ up to their ordering,
    including their names,
    $\varphi_A$ will order them in the same way.
    Thus, the functions in $A \circ B$ and $A \circ B'$.

    In the second case, the functions are of the form:
    $$
    B.\tx{f}_i[\varphi_B(C.f_1), \ldots]
    $$
    Now, $\pi_{\tx{in}}$ is the same for both $B$ and $B'$, as we've remarked
    before.
    Thus, $\varphi_B$ and $\varphi_{B'}$ are the same.
    Thus, the functions in $B \circ C$ are the same as $B \circ C'$,
    up to reordering, as required.

    Having noted all of these points, we can conclude our proof.

    $\blacksquare$
\end{lemma}

Now, we look at the other kind of composition for packages:
tensoring.
The intuitive idea is that tensoring allows us to run two
packages ``in parallel''.
The result of tensoring two packages is a new package with the functions
in both packages, allowing us to interact with one package or the other at will.
We'll discuss the semantics a bit more after the formal definition.

\begin{definition}[Package Tensoring]
    Given two packages $A$, $B$, with $\tx{Out}(A) \cap \tx{Out}(B) = \emptyset$,
    we can define their tensoring $A \otimes B$ as a package characterized by:
    \begin{itemize}
        \item a state type $(A.S, B.S)$,
        \item input names $\tx{In}(A) \cup \tx{In}(B)$,
        \item output names $\tx{Out}(A) \cup \tx{Out}(B$),
        \item an output name assignment defined by:
        $$
        \pi_{\tx{out}}(i) := \begin{cases}
            A.\pi_{\tx{out}}(i) & i \leq A.n\cr
            A.n + B.\pi_{\tx{out}}(i - A.n) & i > A.n
        \end{cases}
        $$
        \item an input index assignment $\pi_{\tx{in}}(n)$ which
        returns the index of $n$ in the list of names $\tx{In}$, sorted
        in lexicographic order.
    \end{itemize}

    Then, for the functions, we have two cases.
    We use a common helper function:
    $$
    \begin{aligned}
    &\tx{lift}_1(f) := (((s_1, s_2), s), i) \mapsto (s'_1, o) \gets f(s_1, i); (((s'_1, s_2), s), o)\cr
    &\tx{lift}_2(f) := (((s_1, s_2), s), i) \mapsto (s'_2, o) \gets f(s_2, i); (((s_1, s'_2), s), o)
    \end{aligned}
    $$
    for $i \in {1, 2}$ to lift
    a function operating on one side of the state to operate on the whole state.

    For $i \in [1, \ldots, A.n]$, we have:
    $$
    f_i := (h_1, \ldots, h_m) \mapsto
    \tx{lift}_1(A.f_i[h_{\pi_\tx{in}(A.\pi_{\tx{in}}^{-1}(j))}\mid j \in [A.m]])
    $$
    Then, for $i \in [A.n + 1, \ldots, A.n + B.n]$, we have:
    $$
    f_i := (h_1, \ldots, h_m) \mapsto
    \tx{lift}_2(B.f_i[h_{\pi_\tx{in}(B.\pi_{\tx{in}}^{-1}(j))}\mid j \in [B.m]])
    $$

    $\square$
\end{definition}

The state of $A \otimes B$ is just the state of both packages,
and $A \otimes B$ also takes in the inputs of both packages,
which may overlap, and produces the output functions of both packages.
We require that these output functions do not overlap, to make
it clear which function belongs to which ``side'' of the package.

Defining the output functions requires a little bit of technical juggling.
One detail is that we start with functions expecting to receive
just their state, but need to augment them to receive both states,
and then place the result on the corresponding side.
Another technical detail of our formalism shows up here as well,
since $A.f_i$ and $B.f_i$ are parameterized functions, which pick up
an extra state term $s$ after being instantiated with their inputs,
and so $\tx{lift}_i$ needs to also carry this term around.
We also choose to arrange the output functions by $A$ first,
and then $B$, but the order we've chosen is arbitrary.

Now, the trickier details relate to the input functions.
The basic issue is that we need to change the functions so that
they technically accept all the input functions of $A \otimes B$,
but ignore the ones irrelevant to either $A$ or $B$.
We do this by choosing an ``arbitrary'' permutation for $\pi_{\tx{in}}$,
and then pass in the right inputs to $A$ or $B$ by using
their input permutations backwards, allowing us to look up the name
associated with a given index,
which we then use to figure out the right index according to $\pi_{\tx{in}}$.

We choose $\pi_{\tx{in}}$ to be the lexicographic ordering,
because it's a consistent ordering which does not depend
on either $A$ or $B$, and also doesn't care about the order in which
packages are composed.
This technically introduces a new assumption about names, since we
haven't assumed anything about what a name is yet.
However, assuming that names can be sorted alphabetically is not
a strong assumption.

Continuing our analogy of machines, we can see the tensoring of $A \otimes B$
as having two independent machines, side-by-side, that one can interact
with at will.
The state of one machine doesn't interfere with the state of the other,
although both machines might be connected to some common machine ``behind''
them, through composition.

Like with composition, tensoring is also associative.

\begin{lemma}[Tensoring is Associative]
    Given packages $A$, $B$, $C$, it holds that:
    $$
    A \otimes (B \otimes C) \equiv (A \otimes B) \otimes C
    $$
    provided these expressions are well defined.

    \txbf{Proof:} The state types are $(A.S, (B.S, C.S))$
    and $((A.S, B.S), C.S)$, which are isomorphic.

    The input names are $\tx{In}(A) \cup \tx{In}(B) \cup \tx{In}(C)$
    on both sides,
    and the output names are $\tx{Out}(A) \cup \tx{Out}(B) \cup \tx{Out}(C)$
    for both sides as well.

    Next, we get to the crux of the proof, which looks at the functions.

    First, some observations about $\tx{lift}_{i}(\tx{lift}_j(f))$.
    These compositions can always be written in terms of a tuple with
    3 elements:
    $$
    \tx{lift}'_j(f) := (((s_1, s_2, s_3), s), i) \mapsto (s_j, o) \gets f(s_j, i);\ (((s_1, s_2, s_3), s), o)
    $$
    The relation between them is that:
    $$
    \begin{aligned}
    &\tx{lift}_1(\tx{lift}_1(f)) = \tx{lift}'_1(f)\cr
    &\tx{lift}_1(\tx{lift}_2(f)) = \tx{lift}'_2(f)\cr
    &\tx{lift}_2(\tx{lift}_1(f)) = \tx{lift}'_2(f)\cr
    &\tx{lift}_2(\tx{lift}_2(f)) = \tx{lift}'_3(f)\cr
    \end{aligned}
    $$
    So, in both $A \otimes (B \otimes C)$, and $(A \otimes B) \otimes C$, the functions will be of one of three forms:
    \begin{enumerate}
        \item $\tx{lift}_1(A.f_i[\ldots])$,
        \item $\tx{lift}_2(B.f_i[\ldots])$,
        \item $\tx{lift}_3(C.f_i[\ldots])$.
    \end{enumerate}

    The order of the functions will actually be the same in both cases.

    The only remaining difference, potentially, is the instantiation.
    But, our definition ensures that the instantiation depends only on the names
    of the functions, and these are the same in both cases,
    so we conclude that the functions are equal.

    $\blacksquare$
\end{lemma}

Like with composition, associativity lets us forget about the way we
group multiple tensorings together, letting us simply write $A \otimes B \otimes C$.

Tensoring also satisfies an additional property compared to composition.
Because tensoring just provides the functions of both packages,
it shouldn't actually matter which order we tensor packages together,
since the resulting functions are the same.

\begin{lemma}[Tensoring is Commutative]
    Given packages $A$, $B$, it holds that:
    $$
    A \otimes B \equiv B \otimes A
    $$
    provided these expressions are well defined.

    \txbf{Proof:} The state on the left is $(A.S, B.S)$, and $(B.S, A.S)$
    on the right.
    These states are isomorphic, as we've seen before.

    Similarly, since $\cup$ is commutative, $\tx{In}$ and $\tx{Out}$
    will match on both sides.

    The inputs to each of the functions depend only on the set
    of names of the input functions, which are identical for both sides.
    The ordering is different though, but it suffices
    to swap $f_i$ with $f_{i + A.n}$ to make the ordering match.

    Thus, we conclude that the two packages are the same.

    $\blacksquare$
\end{lemma}

So far, we've treated composition and tensoring as two separate operations,
but very often we want to use them together:
this allows us to decompose a large package
into smaller components, using tensoring and composition.
Then we'll rearrange these components around to make proving
certain properties easier.

One key observation making this kind of rearrangement easier
is related to how tensoring and composition interact with each other.

\begin{lemma}[Interchange Lemma]
    \label{thm:package_interchange}
    Given packages $A$,$B$, $C$, $D$, such that
    $\tx{In}(A) \cap \tx{Out}(D) = \emptyset$ and $\tx{In}(C) \cap \tx{Out}(B) = \emptyset$
    $$
    \begin{pmatrix}
        A\cr
        \otimes\cr
        C\cr
    \end{pmatrix}
    \circ
    \begin{pmatrix}
        B\cr
        \otimes\cr
        D\cr
    \end{pmatrix}
    \equiv
    \begin{matrix}
        (A \circ B)\cr
        \otimes\cr
        (C \circ D)\cr
    \end{matrix}
    $$

    \txbf{Proof:} The state on the left is $((A.S, C.S), (B.S, D.S))$,
    while the state on the right is $((A.S, B.S), (C.S, D.S))$.
    These states are isomorphic, of course.

    Now, let's look at $\tx{In}$ and $\tx{Out}$.
    On the left, we have:
    $$
    \tx{In}\left(
    \begin{pmatrix}
        A\cr
        \otimes\cr
        C\cr
    \end{pmatrix}
    \circ
    \begin{pmatrix}
        B\cr
        \otimes\cr
        D\cr
    \end{pmatrix}
    \right)
    =
    \tx{In}
    \begin{pmatrix}
        B\cr
        \otimes\cr
        D\cr
    \end{pmatrix}
    = \tx{In}(B) \cup \tx{In}(D)
    $$
    On the right, we have:
    $$
    \tx{In}
    \begin{pmatrix}
        (A \circ B)\cr
        \otimes\cr
        (C \circ D)\cr
    \end{pmatrix}
    =
    \tx{In}(A \circ B) \cup \tx{In}(C \circ D)
    = \tx{In}(B) \cup \tx{In}(D)
    $$

    For $\tx{Out}$, on the left we have:
    $$
    \tx{Out}\left(
    \begin{pmatrix}
        A\cr
        \otimes\cr
        C\cr
    \end{pmatrix}
    \circ
    \begin{pmatrix}
        B\cr
        \otimes\cr
        D\cr
    \end{pmatrix}
    \right)
    =
    \tx{Out}
    \begin{pmatrix}
        A\cr
        \otimes\cr
        C\cr
    \end{pmatrix}
    = \tx{Out}(A) \cup \tx{Out}(c)
    $$
    On the right, we have:
    $$
    \tx{Out}
    \begin{pmatrix}
        (A \circ B)\cr
        \otimes\cr
        (C \circ D)\cr
    \end{pmatrix}
    =
    \tx{Out}(A \circ B) \cup \tx{Out}(C \circ D)
    = \tx{Out}(A) \cup \tx{Out}(C)
    $$

    Now, we look at the functions.

    On the left, we start with functions of the form:
    $$
    \begin{aligned}
    &(h_1, \ldots) \mapsto
    \tx{lift}_1(A.f_i[h_{(A \otimes C).\pi_\tx{in}(A.\pi_{\tx{in}}^{-1}(j))}\mid j \in [A.m]])\cr
    &(h_1, \ldots) \mapsto
    \tx{lift}_2(C.f_i[h_{(A \otimes C).\pi_\tx{in}(C.\pi_{\tx{in}}^{-1}(j))}\mid j \in [C.m]])\cr
    \end{aligned}
    $$
    then, after composing with $B \otimes D$, using our assumption
    that $A$ uses only functions from $B$, and $C$ only functions from $D$,
    we get:
    $$
    \begin{aligned}
    &(h_1, \ldots) \mapsto
    \tx{lift}_1(A.f_i[\tx{lift}_1(\varphi_{A}(B.f_1)[h_{(B \otimes D).\pi_\tx{in}(B.\pi_{\tx{in}}^{-1}(j))}\mid j \in [B.m]]), \ldots])\cr
    &(h_1, \ldots) \mapsto
    \tx{lift}_2(C.f_i[\tx{lift}_2(\varphi_{C}(D.f_1)[h_{(B \otimes D).\pi_\tx{in}(B.\pi_{\tx{in}}^{-1}(j))}\mid j \in [D.m]]), \ldots])\cr
    \end{aligned}
    $$
    This is because in $A \otimes C$, the order parameters are instantiated
    depends only on the names of the function, and so the order
    will correspond with that of $\varphi_A$ or $\varphi_C$, respectively.

    From the right, the functions will be of the forms:
    $$
    \begin{aligned}
    &(h_1, \ldots) \mapsto \tx{lift}_1(A.f_i[\varphi_A(B.f_1)[h_{\pi_\tx{in}((A \circ B).\pi_{\tx{in}}^{-1}(j))}\mid j \in [(A \circ B).m]]])\cr
    &(h_1, \ldots) \mapsto \tx{lift}_2(C.f_i[\varphi_C(D.f_1)[h_{\pi_\tx{in}((C \circ D).\pi_{\tx{in}}^{-1}(j))}\mid j \in [(C \circ D).m]]])\cr
    \end{aligned}
    $$
    Now, $(A \circ B).m = B.m$, and ditto for $C \circ D$.
    Furthermore, the $\pi_{\tx{in}}$ used here is the same as $(B \otimes D).\pi_{\tx{in}}$,
    since the function only depends on $\tx{In}(B) \cup \tx{In}(D)$.

    The remaining difference is about
    $$
    (A \otimes C).\tx{lift}_i(f[(B \otimes D).\tx{lift}_i(g)])
    \stackrel{?}{=}
    ((A \otimes C) \circ (B \otimes D)).\tx{lift}_i(f[g])
    $$

    Expanding the right hand side, for $i = 1$, we get:
    $$
    (((s_A, s_B, s_C, s_D), s), i) \mapsto (s_A, s_B, o) \gets f[g](((s_A, s_B), s), i);\ (((s_A, s_B, s_C, s_D), s), o)
    $$
    An equivalent way of writing this would be:
    $$
    \begin{aligned}
    &((((s_A, (s_B, s_D)), s_C), s), i) \mapsto\cr
    &\quad ((s_A, (s_B, s_D)), o) \gets f[\tx{lift}_1(g)](((s_A, (s_B, s_D)), s), i)\cr
    &\quad ((((s_A, (s_B, s_D)), s_C), s), o)
    \end{aligned}
    $$
    But this is just $\tx{lift}_1(f[\tx{lift}_1(g)])$.
    A similar argument works for $i = 2$ as well.

    Having eliminated all differences between the functions for the packages
    we're comparing, we conclude our proof.

    $\blacksquare$
\end{lemma}

This proof marks the last very technical proof using the formal definition
of packages.
We've now developed almost all of the machinery we need to start
reasoning about packages syntactically, using the fundamental
operations and properties we've just defined.

We do need one more gadget though, which allows us to easily
thread functions around.

\begin{definition}[Identity Packages]
    Given a set of names $N$, we can define the identity package
    $1(N)$ as a package characterized by:
    \begin{enumerate}
        \item A state $S := \emptyset$,
        \item $\tx{In} = N$,
        \item $\tx{Out} = N$,
        \item $\pi_{\tx{in}} = \pi_{\tx{out}}^{-1}$, based on a lexicographical
        ordering of $N$,
        \item Functions $f_1, \ldots, f_{|N|}$ defined via:
        $$
        f_i := (h_1, \ldots, h_m) \mapsto h_i
        $$
    \end{enumerate}

    $\square$
\end{definition}

In other words, the identity package $1(N)$ simply uses some functions,
and provides them without any changes whatsoever.
This means that $1(\tx{Out}(A)) \circ A \equiv A$,
and $B \circ 1(\tx{In}(B)) = B$, which is why we call this an identity
package.

On its own, this might not seem all that useful, but it becomes essential
when combined with tensoring, allowing us to define packages such as:
$$
\begin{pmatrix}
    A\cr
    \otimes\cr
    1(\tx{Out}(B))
\end{pmatrix}
\circ B
$$
Here, $B$ is used both by $A$, but its functions are also forwarded further.
This kind of arrangement is very useful when defining packages.

We also have a few pieces of shorthand that are useful for identity packages.
We write $1(A, B, \ldots)$ for $1(A \cup B \cup \ldots)$,
and we also sometimes abuse notation to write
$1(P)$ where $P$ is a package, to mean $1(\tx{Out}(P))$,
since forwarding the entire output of a package is a very common operation.

\subsection{Indistinguishability and Reductions}

The goal of this subsection is to define more useful notions of equality.
Literal equality is far too strict, since it will not allow
for many modifications which yield packages that are \emph{effectively}
the same.
Furthermore, in many situations, we want to consider packages
that are hard to tell apart with limited computational resources;
such ``hard problems'' are the basis of many cryptographic schemes.
Furthermore, we want to relate the hardness of distinguishing
one pair of packages to the hardness of distinguishing another pair:
this is the notion of \emph{reduction}.

First, we need to extend our notion of \emph{efficiency}
from functions to packages.

\begin{definition}[Efficient Packages]
    A package $P$ is said to be \emph{efficient} if all of its
    functions are efficient.

    In turn, a parameterized function $f$ is \emph{efficient}
    if for any efficient functions $h_1, \ldots$, the instantiation
    $f[h_1, \ldots]$ is also efficient.

    $\square$
\end{definition}

This is a very natural definition of efficiency,
and one can verify that efficiency is preserved under both
tensoring and composition.

The next notion we define is that of the \emph{game}.
\begin{definition}[Game]
    A game $G$ is a package with $\tx{In}(G) = \emptyset$.

    $\square$
\end{definition}

This is a very simple distinction, but it's important,
because when a package has no input functions, then one can interact
with it as a complete machine already, there's nothing that needs
to be plugged in before the machine can actually ``run''.

The next fundamental notion we define is that of the \emph{adversary}.
Intuitively, adversaries are trying to distinguish games with the same
interface apart.
A ``hard'' problem can be characterized by a pair of games that
no efficient adversary can tell apart.

\begin{definition}[Adversaries]
    An adversary $\mathcal{A}$ for a package $P$, is a package
    with no state, $\tx{In}(\mathcal{A}) = \tx{Out}(P)$, and $\tx{Out}(\mathcal{A}) = \{\tx{run}\}$,
    where $\tx{run} : \randf{\bullet}{\bin}$.

    $\square$
\end{definition}

We'll use adversaries to define some notions of indistinguishability
for games first, but we already define adversaries as being for \emph{packages},
to be ready for when we extended these notions later.

We can think of an adversary as playing a ``game'' of distinguishing
between two packages.
The goal of an adversary is to separate the two packages,
by returning $0$ in one case, and $1$ in the other.
The success of an adversary will be measured by how often it's able
to distinguish the two packages.

Another point of view is that an adversary $\mathcal{A}$ is actually
a mapping from games with a given interface to \emph{functions}
of type $\randf{\bullet}{\bin}$.
Each game we feed to the adversary yields a different function.
This is particularly convenient because we've already developed
notions of equality and distance for functions, and we can use
this mapping to lift the notions to packages as well.

This leads to our next definition:
\begin{definition}[Adversarial Distance]
    Given two games $G$, $H$ with $\tx{Out}(G) = \tx{Out}(H)$, and
    an adversary $\mathcal{A}$ for $G$ or $H$, we define their adversarial
    distance relative to $\mathcal{A}$ as:
    $$
    \varepsilon_{\mathcal{A}}(G, H) := \varepsilon(\mathcal{A} \circ G, \mathcal{A} \circ H)
    $$
    Here we abuse notation a bit to let $\mathcal{A} \circ X$ denote the
    \emph{function} we get by calling $\tx{run}$.

    $\square$
\end{definition}

As the name suggests, this relation also forms a distance metric.

Like with functions, this also leads to a natural notion of equality
for games. But first, to avoid having to say $\tx{Out}(G) = \tx{Out}(H)$ many times,
we define the following shorthand:
\begin{definition}[Game Shape]
    Two games $G$,$H$ are said to have the \emph{same shape} if
    $\tx{Out}(G) = \tx{Out}(H)$.

    $\square$
\end{definition}

We can then continue with our definition of equality.

\begin{definition}[Game Equality]
    Given two games $G$ and $H$ with the same shape, we say
    that $G$ and $H$ are \emph{equal}, written $G = H$,
    if for all adversaries $\mathcal{A}$, we have:
    $$
    \varepsilon_{\mathcal{A}}(G, H) = 0
    $$

    $\square$
\end{definition}
Note that we consider all adversaries, even potentially unbounded ones.
Because adversarial distance is a metric, we also immediately
conclude that this relation is a valid equality relation.

We've intentionally used the $=$ symbol here, because we think that
this is the most natural notion of equality for games.
It allows for inessential differences to be ignored,
such as two ways of sampling from the same distribution,
but it's also not too loose of a notion either, since we consider
\emph{unbounded} adversaries.
Any tangible difference in distributions can be sniffed out by
such a powerful adversary.

We do nonetheless want to develop a looser notion of equality,
which can both allow for a small possibility of success
in distinguishing two games, as well as the possibility of genuinely
hard problems, by restricting the resources of the adversary.

First, we need to define the kind of upper bound we use
to characterize this success probability.
Often, this can just be a number, but we also need to generalize
the notion in order to be able to handle \emph{reductions} as well.

\begin{definition}[Advantage Bound]
    An advantage bound $\epsilon = (f_\epsilon, \epsilon_b^1, \ldots, \epsilon_b^n)$
    consists of an increasing function $f_\epsilon : \mathbb{R}^n \to \mathbb{R}$,
    along with $n$ pairs of games $\epsilon^1_0, \epsilon^1_1, \ldots$, with
    $\epsilon^i_0$ and $\epsilon^i_1$ having the same shape.

    $\square$
\end{definition}

We'll have more to say about this definition, and how it relates to reductions
later.
For now, note that the case of a single number is captured by setting
$f_\epsilon(\ldots) = \alpha$, for some constant $\alpha$.
Next, we look at how we can use this notion of an advantage bound
to define \emph{indistinguishability}.

\begin{definition}[Game Indistinguishability]
    Given two games $G$ and $H$ with the same shape,
    we say that $G$ and $H$ are indistinguishable up to an advantage bound $\epsilon$,
    written ${G \overset{\epsilon}{\approx} H}$, if for all \emph{efficient}
    adversaries $\mathcal{A}$, and for sufficiently large $\lambda$\footnote{
        By this we mean that there exists a $\lambda_0$ (which can depend on $\mathcal{A}$) such that $\forall \lambda \geq \lambda_0$, the inequality holds, noting that $\mathcal{A}, G, H, \epsilon$ are all implicit functions of $\lambda$.
        We thank Joseph Jaeger for pointing out the necessity of this condition,
        and some other flaws in defining reductions in a previous version of this paper.
    } there exists adversaries $\mathcal{B}_1, \ldots, \mathcal{B}_n$ such that:
    $$
    \varepsilon_{\mathcal{A}}(G, H) \leq f_\epsilon(\varepsilon_{\mathcal{B}_1}(\epsilon^1_0, \epsilon^1_1), \ldots, \varepsilon_{\mathcal{B}_2}(\epsilon^n_0, \epsilon^n_1))
    $$

    $\square$
\end{definition}

This definition only considers efficient adversaries to allow for
hard problems to exist, and also allows a bit of a ``gap'',
letting the adversary have some success at distinguishing
the two games, related to the success of adversaries in other games.

The purpose of these two definitions are to capture reductions.
When we say that distinguishing a pair of games $G_0, G_1$ reduces
to distinguishing a pair of games $H_0, H_1$, we mean that
$G_b$ is at least as hard as $H_b$, in the sense that
any attack against $G_b$ can be converted to an attack against $H_b$,
with some reasonable relationship on the success probability.

More formally, a reduction is a statement of the form:
``for all (efficient) adversaries $\mathcal{A}$
against $G_b$, there exists an (efficient) adversary $\mathcal{B}$
against $H_b$, such that $\varepsilon_{\mathcal{A}}(G_0, G_1)$
is at most $\varepsilon_{\mathcal{B}}(H_0, H_1)$''.
This statement is interesting because if $\varepsilon_{\mathcal{B}}$
is ``small'', then $\varepsilon_{\mathcal{A}}$ will also be ``small''.
So, $G_b$ is hard assuming $H_b$ is.

The way we'd translate this statement into the definitions we've
seen is by using the bound $\epsilon := (x \mapsto x, H_b)$.
Our reduction is then a statement that $G_0 \overset{\epsilon}{\approx} G_1$.
We'll often use the shorthand $G_0 \overset{H_b}{\approx} G_1$ to denote
this situation.
This kind of bound could be more complicated, involving several games and even
functions of $\lambda$, such as:
$$
\begin{aligned}
&\epsilon := (f_\epsilon, A_b, C_b),
&f_\epsilon(a, b) := \frac{Q^2}{2^{\lambda}} + 2 \cdot a + \sqrt{b}\cr
\end{aligned}
$$

These advantage bounds can also be added together
in a natural way.

\begin{definition}[Advantage Bound Addition]
    Given two advantage bounds $\alpha := (f_\alpha, \alpha^1_b, \ldots, \alpha^n_b)$
    and $\beta := (f_\beta, \beta^1_b, \ldots, \beta^m_b)$, we can define
    their addition $\alpha + \beta$ as:
    $$
    \begin{aligned}
    &\alpha + \beta := (f_{(\alpha + \beta)}, \alpha^1_b, \ldots, \alpha^n_b, \beta^1_b, \ldots, \beta^m_b)\cr
    &f_{(\alpha + \beta)}(a^1, \ldots, a^n, b^1, \ldots, b^m) := f_\alpha(a^1, \ldots, a^n) + f_\beta(b^1, \ldots, b^m)
    \end{aligned}
    $$
    $\square$
\end{definition}

(This definition even extends to any increasing $u : \mathbb{R}^2 \to \mathbb{R}$).

We can use this addition of bounds to define a notion of transitivity
for indistinguishability.

\begin{lemma}[Transitivity of Indistinguishability]
    Given games $G$, $H$, $I$ satisfying:
    $$
    G \overset{\epsilon_1}{\approx} H, \quad H \overset{\epsilon_2}{\approx} I
    $$
    it holds that:
    $$
    G \overset{\epsilon_1 + \epsilon_2}{\approx} I
    $$
    In more detail, for all efficient adversaries $\mathcal{A}$, we have:
    $$
    \varepsilon_{\mathcal{A}}(G, I) \leq \epsilon_1 + \epsilon_2
    $$

    \txbf{Proof:} Since $\varepsilon_{\mathcal{A}}$ is a metric, it
    satisfies the triangle inequality, so we have:
    $$
    \varepsilon_{\mathcal{A}}(G, I) \leq \varepsilon_{\mathcal{A}}(G, H) + \varepsilon_{\mathcal{A}}(H, I)
    $$
    Then, we just need to apply our assumptions to get the upper bound we 
    need to prove.

    $\blacksquare$
\end{lemma}

This notion of transitivity is very useful, since it lets us argue
that two different games are equal by appealing to several successive
differences.
For example, some system might use both encryption and signing,
and we can appeal to the hardness of both problems, one at a time,
to argue that the system is secure.
This kind of technique is called ``game hopping'',
and one of the strengths of state-separable proofs is making
the application of the technique as simple and routine as possible.

Having defined these notions of equality for games,
we now extend them to \emph{packages}.
The natural way to do this is by trying to turn
a package into a game, and then using the notions we've just developed.

Let's look at a way to do this transformation.

\begin{definition}[Completion]
    Given a package $A$, a completion of $A$ is a game $\mathcal{C}$,
    such that $\tx{Out}(\mathcal{C}) \supseteq \tx{In}(A)$,
    and $\tx{Out}(\mathcal{C}) \cap \tx{Out}(A) = \emptyset$.

    We write:
    $$
    \tx{Compl}_{\mathcal{C}}(A) := \begin{pmatrix}
        A\cr
        \otimes\cr
        1(\mathcal{C})
    \end{pmatrix}
    \circ \mathcal{C}
    $$

    $\square$
\end{definition}

So, a completion is one way of turning a package into a game.
It does so by filling in all of the input functions,
but it also leaks extra information forward.
The reason behind this is so that an adversary is also able
to see what's happening ``behind'' the package $A$.
Note that for completions, the names of the extra
functions, those in $\tx{Out}(\mathcal{C}) / \tx{In}(A)$,
are very inessential, and should be considered
as being distinct from any other name used by a real package.

Before we extend our notions of equality to packages, we need
to quickly extend our notion of \emph{shape} first.

\begin{definition}[Package Shape]
    Two packages $A$, $B$, are said to have the \emph{same shape}
    if $\tx{Out}(A) = \tx{Out}(B)$, and $\tx{In}(A) = \tx{In}(B)$.

    $\square$
\end{definition}

We're now ready to define equality and indistinguishability for packages.

\begin{definition}[Package Equality and Indistinguishability]
    Given two packages $A$, $B$ with the same shape, we say that:
    \begin{enumerate}
        \item $A$ is equal to $B$, written $A = B$, if for all completions
        $\mathcal{C}$, we have $\tx{Compl}_{\mathcal{C}}(A) = \tx{Compl}_{\mathcal{C}}(B)$,
        \item $A$ is indistinguishable up to $\epsilon$ with $B$, written $A \overset{\epsilon}{\approx} B$,
        if for all \emph{efficient} completions $\mathcal{C}$, we have $\tx{Compl}_{\mathcal{C}}(A) \overset{\epsilon}{\approx} \tx{Compl}_{\mathcal{C}}(B)$.
    \end{enumerate}

    $\square$
\end{definition}

A completion turns a package into a game, so it's natural to compare
packages by using completions.
However, there's no ``canonical'' completion, so it's not clear
which one to use to compare the packages.
We get around this problem by simply using all of them.

One way of looking at these notions of equality is that we
have an adversary which completely surrounds a package $A$,
seeing both the ``front'', via $\tx{Out}(A)$, and the ``back'',
via $\tx{In}(A)$, and can distinguish the package from others by influencing
either side.
This is why it's important that the adversary can interact with $\mathcal{C}$
directly, so that $\mathcal{A}$ and $\mathcal{C}$ effectively form
one unified adversary.

The basic properties of equality, like symmetry and transitivity,
also hold for packages, given the definition in terms of games.

\subsection{Some Properties of Equality}

So far, we've seen three notions of equality:
\begin{enumerate}
    \item Literal Equality ($\equiv$),
    \item Equality ($=$),
    \item Indistinguishability ($\overset{\epsilon}{\approx}$).
\end{enumerate}
We've considered them in isolation, but in fact there's
a very natural link between the three:
each of them is strictly stronger than the other.
We capture this fact in the following theorem.

\begin{lemma}[Equality Hierarchy]
    For any packages $A$, $B$ with the same shape, it holds that:
    \begin{enumerate}
        \item $A \equiv B \implies A = B$,
        \item $A = B \implies A \overset{0}{\approx} B$.
    \end{enumerate}

    \txbf{Proof:} For part one, if $A \equiv B$, then
    $\mathcal{A} \circ \tx{Compl}_{\mathcal{C}}(A) \equiv \mathcal{A} \circ \tx{Compl}_{\mathcal{C}}(B)$, since
    composition and tensoring preserve literal equality.
    But, in that case, by definition of $\equiv$, the $\tx{run}$
    functions must be equal in both cases, which means that:
    $$
    \varepsilon(\mathcal{A} \circ \tx{Compl}_{\mathcal{C}}(A), \mathcal{A} \circ \tx{Compl}_{\mathcal{C}}(B)) = 0
    $$
    which is what we needed to prove.

    For part 2, note that if for \emph{every} adversary $\mathcal{A}$
    and completion $\mathcal{C}$,
    we have:
    $$
    \varepsilon_{\mathcal{A}}(\tx{Compl}_{\mathcal{C}}(A), \tx{Compl}_{\mathcal{C}}(B)) = 0
    $$
    then, in particular, this relation holds for every \emph{efficient} adversary
    and completion as well, which is what we needed to prove.

    $\blacksquare$
\end{lemma}

This hierarchy is quite useful, since we can prove precise
equality relations between packages,
but then ultimately use them in game hopping,
where only $\approx$ matters.
The hierarchy also lets us basically forget about $\equiv$,
since whenever we would've used it, we can just use $=$ instead,
which is applicable to many more packages.

The main properties we need to prove to wrap up our formal discussion
of packages relate to showing that the composition operations we've
defined respect equality and indistinguishability.
This is very important, since it lets us reason about large
packages by arguing that small components are equal or indistinguishable,
and will form the crux of most proofs.

We start with tensoring, since the proof is simpler.

\begin{lemma}[Tensoring Respects Equality]
    \label{thm:pack_tensoring_respect}
    Given packages $A$, $B$, $B'$, it holds that:
    \begin{enumerate}
        \item $B = B' \implies A \otimes B = A \otimes B'$,
        \item $B \overset{\epsilon}{\approx} B' \implies A \otimes B \overset{\epsilon}{\approx} A \otimes B'$.
    \end{enumerate}
    provided that these expressions are well defined,
    and, for part 2, that $A$ is efficient.

    \txbf{Proof:}

    \txbf{1.}
    Let $\mathcal{C}$ be some completion for $A \otimes B$.
    We have:
    $$
    \tx{Compl}_{\mathcal{C}}(A \otimes B)
    =
    \begin{pmatrix}
        A\cr
        \otimes\cr
        B\cr
        \otimes\cr
        1(\mathcal{C})
    \end{pmatrix}
    \circ \mathcal{C}
    $$
    Now, we apply interchange to write this as:
    $$
    \begin{pmatrix}
        A\cr
        \otimes\cr
        1(B)\cr
        \otimes\cr
        1(\mathcal{C})
    \end{pmatrix}
    \circ
    \begin{pmatrix}
        B\cr
        \otimes\cr
        1(\mathcal{C})
    \end{pmatrix}
    \circ \mathcal{C}
    = W \circ \tx{Compl}_{\mathcal{C}}(B)
    $$
    for some package $\mathcal{W}$.
    For any adversary $\mathcal{A}$, we have:
    $$
    \varepsilon_{\mathcal{A}}(\tx{Compl}_{\mathcal{C}}(B), \tx{Compl}_{\mathcal{C}}(B')) = 0
    $$
    In particular, for any adversary $\mathcal{A}'$ against $\tx{Compl}_{\mathcal{C}}(A \otimes B)$,
    we can apply this observation to $\mathcal{A}' \circ W$, giving us:
    $$
    \varepsilon_{\mathcal{A}'}(W \circ \tx{Compl}_{\mathcal{C}}(B), W \circ \tx{Compl}_{\mathcal{C}}(B')) = 0
    $$
    Since this observation holds for any $\mathcal{A}'$, we infer that:
    $$
    W \circ \tx{Compl}_{\mathcal{C}}(B) =
    W \circ \tx{Compl}_{\mathcal{C}}(B')
    $$
    Then, applying transitivity, we conclude that:
    $$
    \tx{Compl}_{\mathcal{C}}(A \otimes B) =
    \tx{Compl}_{\mathcal{C}}(A \otimes B')
    $$

    \txbf{2.} We apply the observation we had above, which is that:
    $$
    \tx{Compl}_{C}(A \otimes B) = W \circ \tx{Compl}_C(B)
    $$
    (and similarly for $B'$). 
    Now, by assumption
    for any efficient adversary $\mathcal{A}$, there exists $\mathcal{B}_1, \ldots$ such that:
    $$
    \varepsilon_{\mathcal{A}}(\tx{Compl}_{\mathcal{C}}(B), \tx{Compl}_{\mathcal{C}}(B')) \leq f_\epsilon(\varepsilon_{\mathcal{B}_1}(\epsilon^1_b), \ldots)
    $$
    for sufficiently large $\lambda$.

    In particular, we can apply this to $\mathcal{A}' \circ W$, for any
    adversary $\mathcal{A}'$ against $A \otimes B$,
    since $W$ is efficient, by virtue of $A$ being efficient.
    This gives us:
    $$
    \varepsilon_{\mathcal{A}'}(W \circ \tx{Compl}_{\mathcal{C}}(B), W \circ \tx{Compl}_{\mathcal{C}}(B')) \leq f_\epsilon(\varepsilon_{\mathcal{B}_1}(\epsilon^1_b), \ldots)
    $$
    for some $\mathcal{B}_1, \ldots$, and all sufficiently large $\lambda$.

    This means that:
    $$
    W \circ \tx{Compl}_{\mathcal{C}}(B) \overset{\epsilon}{\approx} W \circ \tx{Compl}_{\mathcal{C}}(B')
    $$
    We then use transitivity to conclude that:
    $$
    A \otimes B \overset{\epsilon}{\approx} A \otimes B'
    $$

    $\blacksquare$
\end{lemma}

Next, we prove the same kind of theorem about composition.

\begin{lemma}[Composition Respects Equality]
    Given packages $A$, $B$, $B'$, $C$, it holds that:
    \begin{enumerate}
        \item $B = B' \implies A \circ B = A \circ B'$,
        \item $B \overset{\epsilon}{\approx} B' \implies A \circ B \overset{\epsilon}{\approx} A \circ B'$,
        \item $B = B' \implies B \circ C = B' \circ C$,
        \item $B \overset{\epsilon}{\approx} B' \implies B \circ C \overset{\epsilon}{\approx} B \circ C$,
    \end{enumerate}
    provided that these expressions are well defined, and for parts 2 and 4,
    that $A$ and $C$ are efficient, respectively.

    \txbf{Proof:}

    \txbf{1.} For any completion $\mathcal{C}$, we can write:
    $$
    \tx{Compl}_{\mathcal{C}}(A \circ B) =
    \begin{pmatrix}
        A \circ B\cr
        \otimes\cr
        1(\mathcal{C})
    \end{pmatrix}
    \circ \mathcal{C}
    =
    \begin{pmatrix}
        A\cr
        \otimes\cr
        1(\mathcal{C})
    \end{pmatrix}
    \circ
    \begin{pmatrix}
        B\cr
        \otimes\cr
        1(\mathcal{C})
    \end{pmatrix}
    \circ \mathcal{C}
    $$
    by applying interchange.
    We can write this as:
    $$
    W \circ \tx{Compl}_{\mathcal{C}}(B)
    $$
    for some package $W$ depending on $A$ and $\tx{Out}(\mathcal{C})$.

    Then, we apply a similar logic as in our proof of Lemma~\ref{thm:pack_tensoring_respect}.
    For any adversary $\mathcal{A}$, we have:
    $$
    \varepsilon_{\mathcal{A}}(\tx{Compl}_{\mathcal{C}}, \tx{Compl}_{\mathcal{C}}(B')) = 0
    $$
    Thus, for any $\mathcal{A}'$ against $A \circ B$, we apply the above
    to $\mathcal{A}' \circ W$, getting:
    $$
    \varepsilon_{\mathcal{A}'}(W \circ \tx{Compl}_{\mathcal{C}}(B), W \circ \tx{Compl}_{\mathcal{C}}(B')) = 0
    $$
    In other words, we have:
    $$
    W \circ \tx{Compl}_{\mathcal{C}}(B) = W \circ \tx{Compl}_{C}(B')
    $$
    We can then apply transitivity to conclude that $A \circ B = A \circ B'$.

    \txbf{2.} We start with the same observation, that:
    $$
    \tx{Compl}_{\mathcal{C}}(A \circ B) = W \circ \tx{Compl}_{\mathcal{C}}(B)
    $$
    for some package $W$.
    By applying our assumption to $\mathcal{A}' \circ W$ for any adversary
    $\mathcal{A}'$ against $A \circ B$, we see that:
    $$
    \varepsilon_{\mathcal{A}'}(W \circ \tx{Compl}_{\mathcal{C}}(B), W \circ \tx{Compl}_{\mathcal{C}}(B')) \leq f_\epsilon(\varepsilon_{\mathcal{B}_1}(\epsilon^1_b), \ldots)
    $$
    for some $\mathcal{B}_1, \ldots$ and all sufficiently large $\lambda$.
    In other words,
    $$
    W \circ \tx{Compl}_{\mathcal{C}}(B) \overset{\epsilon}{\approx} W \circ \tx{Compl}_{C}(B')
    $$
    and then apply transitivity to reach our conclusion.

    \txbf{3.} For any completion $\mathcal{C}$, we can write:
    $$
    \tx{Compl}_{\mathcal{C}}(B \circ C) =
    \begin{pmatrix}
        B \circ C\cr
        \otimes\cr
        1(\mathcal{C})
    \end{pmatrix}
    \circ \mathcal{C}
    =
    \begin{pmatrix}
        B\cr
        \otimes\cr
        1(\mathcal{C})
    \end{pmatrix}
    \circ
    \begin{pmatrix}
        1(B) \circ C\cr
        \otimes\cr
        1(\mathcal{C})
    \end{pmatrix}
    \circ \mathcal{C}
    $$
    We can then see $C$ as part of a new completion, writing:
    $$
    \begin{pmatrix}
        B\cr
        \otimes\cr
        1(\mathcal{C}')
    \end{pmatrix}
    \circ \mathcal{C}'
    = \tx{Compl}_{\mathcal{C}'}(B)
    $$
    But, by assumption, we have:
    $$
    \tx{Compl}_{\mathcal{C}'}(B) = 
    \tx{Compl}_{\mathcal{C}'}(B')
    $$
    We then apply our initial observation in reverse, along with transitivity,
    to reach our conclusion.

    \txbf{4.} Same as above, except our assumption gives us:
    $$
    \tx{Compl}_{\mathcal{C}'}(B) \overset{\epsilon}{\approx}
    \tx{Compl}_{\mathcal{C}'}(B')
    $$
    and then transitivity can be applied to reach our result once again.

    $\blacksquare$
\end{lemma}

These lemmas form the conceptual crux of how proofs in the state-separable
style work.
If you want to prove that two large packages
are indistinguishable, you do so by a series of observations,
each of which breaks down the package as a composition
of many smaller packages.
Sometimes you'll be able to use theorems you've already proved,
or problems assumed to be hard, in order to argue
that small pieces are indistinguishable, and thus apply
the lemmas we've just demonstrated in order
to lift that indistinguishability to the large composition.
By applying a series of such hops, you eventually
produce a reduction of the security of this large package
to that of several smaller packages.

\subsection{Syntactical Conventions for Packages}

In the previous subsections, we developed a formal model of
how packages work.
In practice, packages are described using a kind of pseudo-code,
which corresponds with these formal objects.
Some of the rules governing packages are also relaxed
in practice.
In this subsection we give some examples of how this pseudo-code
works.
Note that the details here aren't essential, and one could imagine
using a different kind of pseudo-code instead.

We start with an example package, containing various syntactical constructs,
which we'll then explain in more detail.

\package{P}{
    &\draw{k}{\bin^{\lambda}}\cr
    &b \gets \bot\cr
    &\txbf{view } l \gets \tx{List}.\tx{new}()\cr
    \cr
    &\begin{aligned}
    &\pfn{A}{x}\cr
    \pind{1} \txbf{assert } b \neq \bot\cr
    \pind{1} \txbf{return } \tx{Inc}(x)\cr
    &\cr
    &\pfn{B}{}\cr
    \pind{1} b \gets \txt{true}\cr
    \pind{1} x \gets 2\cr
    \pind{1} \pif{b = \txt{false}}\cr
    \pind{2} x \gets x - 1\cr
    \pind{1} \pelse\cr
    \pind{2} x \gets \tx{Inc}(x)\cr
    \pind{1} \preturn{x}\cr
    \end{aligned}
    \begin{aligned}
        &\pfn{Inc}{x}\cr
        \pind{1} \txbf{return } x + 1\cr
        \cr
        &\pfn{$(1)C$}{}\cr
        \pind{1} \draw{x}{[10]}\cr
        \pind{1} \pwhile{x > 0}\cr
        \pind{2} x \gets x - 1\cr
        \cr
        \cr
        \cr
        \cr
        \cr
    \end{aligned}
}

So, the basic idea is that a package is usually described
by a box like this, with the name of the package---$P$, in this case---at
the top of the box.
A package has some initialization code,
along with exported functions.
In this case, the exported functions are $A$, $B$, $C$,
$\tx{Inc}$, and $\tx{l}$.

The meaning of $\txbf{view}$ is that the value is exported
in a read only fashion.
So, there's a function $l$ which copies the list $l$
and then returns it.
The caller can modify their copy, but this has no effect on the original
list.

Now, one slight deviation from the formal specification is that
we allow a package to call functions that it exports internally,
like we do for $\tx{Inc}$.
The semantics of this are that the code of $\tx{Inc}$ are inlined
at the call site.
So, in this case, every place where $\tx{Inc}(x)$ is used
can be replaced with just $x + 1$.

We also have standard control flow constructs, like $\txbf{if}$,
$\txbf{else}$, $\txbf{while}$, $\txbf{for}$, $\txbf{return}$, etc.
Functions don't have to return a value,
in which case we assume they return some pre-defined dummy
value, like $\bullet$.

Another construct we have is $\txbf{assert}$.
This should be seen as immediately returning a special value
indicating that an assertion failed, if the condition is indeed
false.
This is useful to restrict when a function can be called.
One very common such restriction is on the number of times
a function can be called.
Since we wrote $(1)C$, we're indicating that this function
can only be called once.
This is shorthand for having a variable keep track of the number
of calls, and an assertion checking that this count is low
enough.

Another slight deviation from the formal specification is the use
of initialization code.
Formally, a package just exports functions,
it doesn't have any code running before those functions do.
One way to add initialization is to have a special function---say, $\txt{Init}$---which must be called before any other of the functions.
The initialization code could then be placed there.

We also don't particularly care about the names and variables
of variables and functions, as long as it's clear which packages
are using what functions.
If it's not ambiguous, we could refer to one of the functions
in $P$ as just $A$,
but we may want to explicitly write down $P.A$, to disambiguate this
function from another, say, $Q.A$.
We might also tensor multiple versions of $P$ together,
calling one function $A_1$, and the other $A_2$, for example.
Another common way to disambiguate names is
to use $\txbf{super}.A$, to refer to calling an input function $A$.

Sometimes, we'll also compare two packages for equality,
with one package exporting more functions than the other.
This is usually shorthand for writing $1(\ldots) \circ P = Q$,
ignoring some of the functions provided by one of the packages.

We stress that these rules are merely conventions,
and are intended to provide a means of clearly expressing
what a package is doing, while also not being excessively verbose.

For further examples of how state-separable proofs work,
we point the reader to the original paper
\cite{AC:BDFKK18}, or
to other works making use of the paradigm
\cite{joyofcryptography, mei22}.
