\section{State-Separable Proofs}

Our framework for describing protocols is based on 
\emph{state-separable proofs}
\cite{AC:BDFKK18}.
The security notions we develop for protocols ultimately
find meaning in analogous notions of security for \emph{packages},
the main object of study in state-seperable proofs.

This section is intended to be a suitable independent presentation
of this formalism.
In that spirit, we develop state-separable proofs ``from scratch''.
Our starting point is merely that of computable randomized functions.
This is in contrast to other protocol security frameworks like UC,
whose foundational starting point is usually the more concrete notion
of \emph{interactive turing machines}.

We also take the opportunity to solidify the formalism of state-separable
proofs, providing more complete definitions of various objects,
completing several proofs left as mere sketches in the original paper,
and proving a few additional properties we'll need later.
This makes this section of interest to readers who are already familiar
with state-separable proofs.

\subsection{Some Notational Conventions}

We write $[n]$ to denote the set $\{1, \ldots, n\}$.

We write $\txt{01}$ to denote the set $\{0, 1\}$,
and write $\str$ to denote binary strings.
We write $\bullet$ to denote the empty string,
which also serves as a ``dummy'' value in various contexts.

\subsection{Probabilistic Functions}

Our starting point is the notion of \emph{randomized computable functions}.
This is a notion we assume can be defined in a rigorous way, but whose
concrete semantics we don't assign.
We write $f : \randf{\str}{\str}$ to denote such a function (named $f$).
Intuitively, this represents a function described by some algorithm,
which takes in a binary string as an input, and produces a binary string
as output, and is allowed to make randomized decisions to aid its computation.

We mainly consider \emph{families} of functions,
parametrized by a security parameter $\lambda$.
Formally, this is in fact a function $f : \mathbb{N} \to \randf{\str}{\str}$,
and we write $f_{\lambda} : \randf{\str}{\str}$ to denote a particular
function in the family.
In most cases, this security parameter is left \emph{implicit}.
In fact, all of the objects we consider from here on out will \emph{implicitly}
be \emph{families} of objects, parametrized by a security parameter $\lambda$,
and we will invoke this fact only as necessary.

\begin{definition}[Efficient Functions]
    We assume that a function $f$ has a runtime, denoted $T(f, x)$,
    measuring how long the function takes to execute on a given input $x \in \str$.

    We say that a function family is \emph{efficient} if:
    $$
    \forall \lambda.\ \forall x, |x| \in \mathcal{O}(\tx{poly}(\lambda)).\quad T(f_\lambda, x) \in \mathcal{O}(\tx{poly}(\lambda))
    $$
    In other words, the runtime is always polynomial in $\lambda$, regardless
    of the input, or the random choices of the function.

    $\square$
\end{definition}

Functions which are not necessarily efficient are said to be \emph{unbounded}.

Considering efficient functions is essential, because the vast majority
of cryptographic techniques depend on assuming that some problems are ``hard''
for adversaries with bounded computational resources,
and so this notion of efficiency is critical to defining game-based
security.
Ironically, for protocol security, many protocols can be proved secure
without this restriction.

Another crucial notion we need to develop is that of a \emph{distance},
measuring how different two functions behave.
This will underpin our later notion of security for games,
which is based on saying that two different games are difficult
to tell apart.

\begin{definition}[Distance Function]
    Given a function $f : \randf{\bullet}{\bin}$, we assume that the probability
    $P[f \to 1]$ of the function returning $1$ on the input $\bullet$ is well defined.

    Given two functions $f, g$, we define their distance $\varepsilon(f, g)$ as:
    $$
    \varepsilon(f, g) := |P[f \to 1] - P[g \to 1]|
    $$

    $\square$
\end{definition}

In other words, the distance looks at how often one function returns $1$
compared to the other.
If the functions agree most of the time, then their distance will be small,
whereas if they disagree very often, their distance will be large.
This definition is actually quite natural.
Since $P[f \to 1] = (1 - P[f \to 0])$, $\varepsilon$ is actually
just the total variation---or statistical---distance.
This immediately implies that this distance has some nice properties,
in particular that it forms a \emph{metric}.

\begin{lemma}[Distance is a Metric]
    $\varepsilon$ is a valid metric, in particular, it holds
    for any functions $f, g, h$, that:
    \begin{enumerate}
        \item $\varepsilon(f, f) = 0$,
        \item $\varepsilon(f, g) = \varepsilon(g, f)$,
        \item $\varepsilon(f, h) \leq \varepsilon(f, g) + \varepsilon(g, h)$.
    \end{enumerate}

    \txbf{Proof:}

    \txbf{1.} Follows from the fact that $P[f \to 1] = P[f \to 1]$,
    so $\varepsilon(f, f) = 0$.

    \txbf{2.} Follows from the fact that $|a - b| = |b - a|$.

    \txbf{3.} Follows from the triangle inequality for $\mathbb{R}$
    and the fact that:
    $$
    |P[f \to 1] - P[h \to 1]| = |(P[f \to 1] - P[g \to 1]) + (P[g \to 1] - P[H \to 1])|
    $$

    $\blacksquare$
\end{lemma}

We actually skipped one property in our proof that $\varepsilon$ is a valid metric,
which requires that if $f \neq g$, then $\varepsilon(f, g) > 0$.
This is because we haven't yet defined what equality should mean
for functions.
This metric property gives us a very natural definition though.

\begin{definition}[Function Equality]
    Two functions, $f$ and $g$, are \emph{equal}, written $f = g$, when:
    $$
    \varepsilon(f, g) = 0
    $$

    $\square$
\end{definition}

It's easy to see that this is an equality relation, satisfying
reflexivity, symmetry, and transitivity.

We can also generalize this to arbitrary functions, rather than just
$f : \randf{\bullet}{\bin}$, by defining:
$$
\varepsilon(f, g) := \tx{sup}_{x, y \in \str} |P[f(x) \to y] - P[g(x) \to y]|
$$
however, we will not really be needing this general definition, outside
of a technical and very strong notion of equality for packages
used in the following subsection.

While the functions we've considered so far only manipulate binary strings,
it's useful to allow \emph{typed} functions,
with richer input and output types.
This could be defined in several ways, but the end result means
that a typed function $f : \randf{A}{B}$ can be interpreted as a function
over binary strings, using a suitable encoding and decoding mechanism,
as well as perhaps having a special output value that $f$ can return
if it fails to decode its input successfully.

Being able to quantify types is also useful for the formalism itself,
and potentially even for some packages.
This allows us to type functions such as:
$$
\begin{aligned}
    &\tx{id} : \forall s.\ s \to s\cr
    &\tx{id} = x \mapsto x
\end{aligned}
$$
In this example, $s$ is a quantified type variable.
Formally, we can see $\tx{id}$ as a function parametrized by a type,
with $\tx{id}_S$ being a concrete function, after having chosen this type.

\subsection{Packages}

Our next goal is to define the central object of state-separable proofs:
the \emph{package}.
Intuitively, a package has some kind of state, as well as functions
which manipulate this state.
You can interact with a package by calling the various output functions
it provides.
This makes packages a natural fit for security games.
What distinguishes packages from games is that they can have \emph{input}
functions.
A package can depend on another package, with each of its functions
potentially using the functions provided by this other package.
This modularity makes the common proof technique of ``game-hopping''
much more easily usable, and is the core strength of the state-separable
proof formalism.

Before we get to packages, we first need to define a few convenient
notions for functions manipulating a state, and parametrizing
functions with other functions.

Our first definition will be a little bit of shorthand.
\begin{definition}[Stateful Function]
    A \emph{stateful} function is simply a function $f$ of the form:
    $$
    f : \randf{(S, \str)}{(S, \str)}
    $$
    $S$ represents the state being used and modified by the function.
    As a convenient shorthand, we write:
    $$
    f :\ \stateful{S}
    $$

    $\square$
\end{definition}
It's useful to have a bit of typing to separate the state from the rest
of the input and output, since it allows us to avoid defining
inessential padding details inside the formalism itself.

We'll also want a notion of equality for these functions.

\begin{definition}[Stateful Function Equality]
    Two stateful functions $f : \stateful{S}$ and $f : \stateful{S'}$
    are equal, written $f = f'$, if there exists an isomorphism
    $\varphi : S \cong S'$, such that:
    $$
    f = (s, i) \mapsto (s', o) \gets f'(\varphi(s), i);\ (\varphi^{-1}(s'), o)
    $$

    $\square$
\end{definition}

Basically, the states don't have to be literally the same, as long
as they're isomorphic, and the natural way of making the two types
match up produces equal functions.
One can verify that this forms a valid equality relation.
Note that this reduces to the standard notion of equality of functions
by considering appropriate binary encodings of the two states.

We also need to consider functions parametrized by other functions.
Intuitively, this arises when one function calls another.
For example, consider:
$$
f(x) := g(x) \oplus g(x)
$$
which is well defined regardless of what $g$ is.
Here $f$ is implicitly parametrized by $g$, but we could write this explicitly
as $f(x) := g \mapsto g(x) \oplus g(x)$.
We could write $f : (\randf{\str}{\str}) \to (\randf{\str}{\str})$  
as a potential type in this example.
We write $f[g]$ for the instantiation of a parametrized function $f$
with an input function $g$.
It might also be the case that $g$ is itself parametrized,
in which case $f[g]$ is defined as:
$$
f[g] := h \mapsto f[g[h]]
$$

We can define a natural, albeit very strong, notion of equality for parametrized
functions, saying that:
$$
f = g \iff \forall h_1, \ldots, h_n.\ f[h_1, \ldots] = g[h_1, \ldots]
$$
In other words, the two functions must be equal regardless of how we instantiate
them.

We've now developed enough tools to define packages.

\begin{definition}[Package]
    A package $A$ consists of:
    \begin{itemize}
        \item a type $S$, for its state,
        \item a set of \emph{input names} $\tx{In}(A)$, of size $m$,
        \item a permutation $\pi_{\tx{in}} : \tx{In}(A) \leftrightarrow [m]$,
        \item a set of \emph{output names} $\tx{In}(A)$, of size $n$,
        \item a permutation $\pi_{\tx{out}} : [m] \leftrightarrow \tx{Out}(A)$,
        \item a set of parametrized functions $f_1, \ldots, f_n : \forall s. \stateful{s}^m \to \stateful{(S, s)}$,
        each of which has a distinct name $n_i \in \tx{Out}(A)$.
    \end{itemize}

    We also only consider a package to be defined \emph{up to} potentially
    renaming its input and output functions injectively.

    $\blacksquare$
\end{definition}

The basic idea is that a package has an internal state $S$, which gets
manipulated by each of the functions it exports.
These functions, in turn, can depend on other input functions.
If a stateful function $f$ uses a stateful function $g$,
then the result is a stateful function $f[g]$ manipulating 
\emph{both} the state of $f$, and the state of $g$.
Furthermore, $f$ is defined in such way agnostic to what the state manipulated
by $g$ happens to be,
which is why we use a \emph{quantified} type instead, to allow instantiation
with functions manipulating different kinds of state.

In practice, each function in a package is unlikely to use \emph{all}
of the input functions of the package, but it is much simpler
to have each function parametrized by all the possible inputs,
even if some are left unused.
It's also much simpler to define an ordering of the input functions
$\pi$, so be able to use $\stateful{s}^m$ as the input type for
the parametrized functions.

The semantics of a package without inputs are intuitively that of a stateful computer program
or machine you can interact with.
The machine has some kind of state, represented by $S$,
along with various functions you can call, represented by $f_1, \ldots, f_n$.
Each of these will use the input you provide, along with the current state
of the machine, in order to supply you with an output,
potentially modifying the state along the way.
The input functions allow a package to interact with other packages itself.

We'll often use $\tx{In}(A)$ or $\tx{Out}(A)$ to talk about the input
and output functions of a package.
As a bit of a short hand notation, we write $\tx{In}(A, B, \ldots)$
for the union $\tx{In}(A) \cup \tx{In}(B) \cup \ldots$, and similarly
for $\tx{Out}(\ldots)$.

We describe this kind of interaction using the formal
notion of package \emph{composition}.

\begin{definition}[Package Composition]
    Given two packages $A, B$ with $\tx{In}(A) \subseteq \tx{Out}(B)$,
    we define their composition $A \circ B$ as a package characterized by:

    \begin{itemize}
        \item a state type $(A.S, B.S)$,
        \item input names $\tx{In}(B)$,
        \item output names $\tx{Out}(A)$,
        \item $\pi_{\tx{in}} := B.\pi_{\tx{in}}$,
        \item $\pi_{\tx{out}} := A.\pi_{\tx{out}}$,
        \item output functions $A.f_1[\varphi(B.f_1), \ldots, \varphi(B.f_{B.n})], \ldots$.
    \end{itemize}

    In more detail, these functions have type $\forall s.\stateful{s}^{B.m} \to \stateful{((A.S, B.S), s)}$,
    and are of the form:
    $$
    (h_1, \ldots, h_{B.m}) \mapsto A.f_i[\varphi_{A}(B.f_1)[h_1, \ldots], \ldots, \varphi_{A}(B.f_{B.n})[h_1, \ldots]]
    $$
    where $\varphi_{A}$ assigns each function $B.f_i$ to a slot in $[m]$
    using $A.\pi_{\tx{in}}$ on the name of that function, $B.n_i$.
    The same input functions $h_j$ being given
    to all the functions used by $A.f_i$.

    $\square$
\end{definition}

Package composition formally defines the intuitive notion of one package
``using'' the functions provided by another package.
The result is a package providing the functions defined in $A$,
and requiring the functions needed by $B$, but with the functions
inside $B$ itself now effectively inlined inside of $A \circ B$.

Next we'd like to prove that package composition satisfies some nice
properties.
For example $A \circ (B \circ C)$ is the same as $(A \circ B) \circ C$.
There's one problem though, which is that we haven't defined
what it means for two packages to be "the same".

\begin{definition}[Literal Equality]
    We say that two packages $A, B$ are \emph{literally equal},
    written $A \equiv B$, when:
    \begin{itemize}
        \item $A.S \cong B.S$,
        \item $\tx{In}(A) = \tx{In}(B)$,
        \item $\tx{Out}(A) = \tx{Out}(B)$,
        \item There exists a permutation $\pi : [n] \leftrightarrow [n]$ such that
        $$\forall i \in [n].\ A.f_i = B.f_{\pi(i)} \land A.n_i = B.n_{\pi(i)}$$
    \end{itemize}

    $\square$
\end{definition}

We require strict equality for the input and output names,
to avoid spurious comparisons between two packages with completely different names,
although it should be noted that packages are only really defined up to renaming anyways,
so this is essentially an isomorphism constraint.
For the type of state, we consider an isomorphism directly,
mainly so that $(A.S, (B.S, C.S))$ is considered to be the same state
type as $((A.S, B.S), C.S)$, which might already be the case depending on
how one defines equality for sets.
The final condition also implies that $\pi_{\tx{in}}$ is the same
for both packages.

This notion of equality is very strong, especially because of the equality
it imposes on the functions defined in each package.
While it suffices to explore basic properties of composition for packages,
we'll want to abandon it quite quickly for a looser and more easily
used notion of equality.

The first property we prove is the one used as an example above.

\begin{lemma}[Associativity of Composition]
    Given packages $A, B, C$, it holds that:
    $$
    A \circ (B \circ C) \equiv (A \circ B) \circ C
    $$
    provided these expressions are well defined.

    \txbf{Proof:} The input and output names are clearly equal on both sides.
    Furthermore, the state on the left is $(A.S, (B.S, C.S))$,
    $((A.S, B.S), C.S)$ on the right, and so the two states are isomorphic.
    All that's left is the final condition, talking about the equality
    of the functions defined in each package.

    Now, for the equality of functions, we'll expand the functions
    of the package on the left, and then on the right, before comparing
    the results we get.

    The functions in $B \circ C$ are of the form:
    $$
    (h_1, \ldots) \mapsto B.f_i[\varphi_{B}(C.f_1)[h_1, \ldots], \ldots]
    $$
    And then the functions in $A \circ (B \circ C)$ are of the form:
    $$
    (h_1, \ldots) \mapsto A.f_i[\varphi_{A}(B.f_1)[\varphi_{B}(C.f_1)[h_1, \ldots]], \ldots]
    $$

    From the other side, the functions in $A \circ B$ are of the form:
    $$
    (h_1, \ldots) \mapsto A.f_i[\varphi_{A}(B.f_1)[h_1, \ldots], \ldots]
    $$
    This makes the functions in $(A \circ B) \circ C$ of the form:
    $$
    (h_1, \ldots) \mapsto A.f_i[\varphi_{A}(B.f_1)[\varphi_{A \circ B}(C.f_1)[h_1, \ldots]], \ldots]
    $$

    The main difference is that we end up with $\varphi_{A \circ B}$
    as our means of assigning the functions in $C$ to the slots
    of $B$.
    However, $\varphi_{X}$ only depends on $X.\pi_{\tx{in}}$,
    and by definition $(A \circ B).\pi_{\tx{in}} = B.\pi_{\tx{in}}$,
    so $\varphi_{A \circ B} = \varphi_B$.

    Another smaller difference is that the resulting stateful functions
    have different, but isomorphic states, which is allowed
    by stateful function equality.

    So, in both cases, we end up with the same functions, concluding
    our proof.

    $\blacksquare$
\end{lemma}

This property is useful, since it lets us simply write $A \circ B \circ C$,
without worrying about the order in which packages are composed.

Another more technical property we want composition to satisfy is
that if \emph{equality preservation}.
If $B \equiv B'$, then it should be the case that $A \circ B \equiv A \circ B'$,
or that $B \circ C \equiv B' \circ C$.
If that weren't the case, then that would indicate that something is wrong
with our definition of either equality or composition.
The property we want for literal equality is that $A$ and $A'$ are completely
interchangeable, and so once can always be replaced with the other, no matter
the context, to the point that we can think of them as literally being the same
package.

Thankfully, it turns out that composition and literal equality do
in fact get along.

\begin{lemma}[Composition Preserves Equality]
    Given any packages $A, B, B', C$ it holds that:
    \begin{itemize}
        \item $B \equiv B' \implies A \circ B \equiv A \circ B'$,
        \item $B \equiv B' \implies B \circ C \equiv B' \circ C$,
    \end{itemize}
    provided these expressions are well defined.

    \txbf{Proof:} In one case the state type is $(A.S, B.S)$
    or $(A.S, B'.S)$, which are isomorphic if $B.S \cong B'.S$.
    Similarly, in the other case, we have $(B.S, C.S)$ vs $(B'.S, C.S)$,
    and the same observation holds.

    Now, remember that $\tx{In}(X \circ Y) = \tx{In}(Y)$, and $\tx{Out}(X \circ Y) = \tx{Out}(X)$.
    Thus, since both $\tx{In}(B) = \tx{In}(B')$ and $\tx{Out}(B) = \tx{Out}(B')$
    hold, we conclude that $\tx{In}$ and $\tx{Out}$ match up in both cases.

    The trickier part is the 4th condition for equality.

    In the first case, the functions are of the form:
    $$
    A.\tx{f}_i[\varphi_A(B.f_1), \ldots]
    $$
    Now, $\varphi_A$ orders the functions in $B$ based only on their \emph{names}.
    In particular, the ordering does not matter.
    Since the functions in $B'$ are the same as $B$ up to their ordering,
    including their names,
    $\varphi_A$ will order them in the same way.
    Thus, the functions in $A \circ B$ and $A \circ B'$.

    In the second case, the functions are of the form:
    $$
    B.\tx{f}_i[\varphi_B(C.f_1), \ldots]
    $$
    Now, $\pi_{\tx{in}}$ is the same for both $B$ and $B'$, as we've remarked
    before.
    Thus, $\varphi_B$ and $\varphi_{B'}$ are the same.
    Thus, the functions in $B \circ C$ are the same as $B \circ C'$,
    up to reordering, as required.

    Having noted all of these points, we can conclude our proof.

    $\blacksquare$
\end{lemma}

Now, we look at the other kind of composition for packages:
tensoring.
The intuitive idea is that tensoring allows us to run two
packages ``in parallel''.
The result of tensoring two packages is a new package with the functions
in both packages, and we can interact with one package or the other at will.
We'll discuss the semantics a bit more after the formal definition.

\begin{definition}[Package Tensoring]
    Given two packages $A$, $B$, with $\tx{Out}(A) \cap \tx{Out}(B) = \emptyset$,
    we can define their tensoring $A \otimes B$ as a package characterized by:
    \begin{itemize}
        \item a state type $(A.S, B.S)$,
        \item input names $\tx{In}(A) \cup \tx{In}(B)$,
        \item output names $\tx{Out}(A) \cup \tx{Out}(B$),
        \item an output name assignment defined by:
        $$
        \pi_{\tx{out}}(i) := \begin{cases}
            A.\pi_{\tx{out}}(i) & i \leq A.n\cr
            A.n + B.\pi_{\tx{out}}(i - A.n) & i > A.n
        \end{cases}
        $$
        \item an input index assignment $\pi_{\tx{in}}(n)$ which
        returns the index of $n$ in the list of names $\tx{In}$, sorted
        in lexicographic order.
    \end{itemize}

    Then, for the functions, we have two cases.
    We use a common helper function:
    $$
    \begin{aligned}
    &\tx{lift}_1(f) := (((s_1, s_2), s), i) \mapsto (s'_1, o) \gets f(s_1, i); (((s'_1, s_2), s), o)\cr
    &\tx{lift}_2(f) := (((s_1, s_2), s), i) \mapsto (s'_2, o) \gets f(s_2, i); (((s_1, s'_2), s), o)
    \end{aligned}
    $$
    for $i \in {1, 2}$, which uses the right side of the state, to lift
    a function operating on one side to operate on the whole state.

    For $i \in [1, \ldots, A.n]$, we have:
    $$
    f_i := (h_1, \ldots, h_m) \mapsto
    \tx{lift}_1(A.f_i[h_{\pi_\tx{in}(A.\pi_{\tx{in}}^{-1}(j))}\mid j \in [A.m]])
    $$
    Then, for $i \in [A.n + 1, \ldots, A.n + B.n]$, we have:
    $$
    f_i := (h_1, \ldots, h_m) \mapsto
    \tx{lift}_2(B.f_i[h_{\pi_\tx{in}(B.\pi_{\tx{in}}^{-1}(j))}\mid j \in [B.m]])
    $$

    $\square$
\end{definition}

The state of $A \otimes B$ is just the state of both packages,
and $A \otimes B$ also takes in the inputs of both packages,
which may overlap, and produces the output functions of both packages.
We require that these output functions do not overlap, to make
it clear which function belongs to which ``side'' of the package.

Defining the output functions requires a little bit of technical juggling.
One detail is that we start with functions expecting to receive
just their state, but need to augment them to receive both states,
and then place the result on the corresponding side.
Another technical detail of our formalism shows up here as well,
since $A.f_i$ and $B.f_i$ are paramterized functions, which pick up
an extra state term $s$ after being instantiated with their inputs,
and so $\tx{lift}_i$ needs to also carry this term around.
We also choose to arrange the output functions by $A$ first,
and then $B$, but the order we've chosen is arbitrary.

Now, the trickier details relate to the input functions.
The basic issue is that we need to change the functions so that
they technically accept all the input functions of $A \otimes B$,
but ignore the ones irrelevant to either $A$ or $B$.
We do this by choosing an ``arbitrary'' permutation for $\pi_{\tx{in}}$,
and then pass in the right inputs to $A$ or $B$ by using
their input permutations backwards, allowing us to look up the name
associated with a given index,
which we then use to figure out the right index according to $\pi_{\tx{in}}$.

We choose $\pi_{\tx{in}}$ to be the lexicographic ordering,
because it's a consistent ordering which does not depend
on either $A$ or $B$, and also doesn't care about the order in which
packages are composed.
This technically introduces a new assumption about names, since we
haven't assumed anything about what a name is yet.
However, assuming that names can be sorted alphabetically is not
a very strong one.

Continuing our analogy of machines, we can see the tensoring of $A \otimes B$
as having two independent machines, side-by-side, that one can interact
with at will.
The state of one machine doesn't interfere with the state of the other,
although both machines might be connected to some common machine ``behind''
them, through composition.

Like with composition, tensoring is also associative.

\begin{lemma}[Tensoring is Associative]
    For any packages $A$, $B$, $C$, it holds that:
    $$
    A \otimes (B \otimes C) \equiv (A \otimes B) \otimes C
    $$
    provided these expressions are well defined.

    \txbf{Proof:} The state types are $(A.S, (B.S, C.S))$
    and $((A.S, B.S), C.S)$, which are isomorphic.

    The input names are $\tx{In}(A) \cup \tx{In}(B) \cup \tx{In}(C)$
    on both sides,
    and the output names are $\tx{Out}(A) \cup \tx{Out}(B) \cup \tx{Out}(C)$
    for both sides as well.

    Next, we get to the crux of the proof, which looks at the functions.

    First, some observations about $\tx{lift}_{i}(\tx{lift}_j(f))$.
    These compositions can always be written in terms of a tuple with
    3 elements:
    $$
    \tx{lift}'_j(f) := (((s_1, s_2, s_3), s), i) \mapsto (s_j, o) \gets f(s_j, i);\ (((s_1, s_2, s_3), s), o)
    $$
    The relation between them is that:
    $$
    \begin{aligned}
    &\tx{lift}_1(\tx{lift}_1(f)) = \tx{lift}'_1(f)\cr
    &\tx{lift}_1(\tx{lift}_2(f)) = \tx{lift}'_2(f)\cr
    &\tx{lift}_2(\tx{lift}_1(f)) = \tx{lift}'_2(f)\cr
    &\tx{lift}_2(\tx{lift}_2(f)) = \tx{lift}'_3(f)\cr
    \end{aligned}
    $$
    So, in both $A \otimes (B \otimes C)$, and $(A \otimes B) \otimes C$, the functions will be of one of three forms:
    \begin{enumerate}
        \item $\tx{lift}_1(A.f_i[\ldots])$,
        \item $\tx{lift}_2(B.f_i[\ldots])$,
        \item $\tx{lift}_3(C.f_i[\ldots])$.
    \end{enumerate}

    The order of the functions will actually be the same in both cases.

    The only remaining difference, potentially, is the instantiation.
    But, our definition ensures that the instantiation depends only on the names
    of the functions, but these are the same in both cases,
    so we conclude that the functions are equal.

    $\blacksquare$
\end{lemma}

Like with composition, associativity lets us forget about the way we
group multiple tensorings together, letting us simply write $A \otimes B \otimes C$.

Tensoring also satisfies an additional property compared to composition.
Because tensoring just provides the functions of both packages,
it shouldn't actually matter which order we tensor packages together,
since the resulting functions are the same.

\begin{lemma}[Tensoring is Commutative]
    For any packages $A$, $B$, it holds that:
    $$
    A \otimes B \equiv B \otimes A
    $$
    provided these expressions are well defined.

    \txbf{Proof:} The state on the left is $(A.S, B.S)$, and $(B.S, A.S)$
    on the right.
    These states are isomorphic, as we've seen before.

    Similarly, since $\cup$ is commutative, $\tx{In}$ and $\tx{Out}$
    will match on both sides.

    The inputs to each of the functions depend only on the set
    of names of the input functions, which are identical for both sides.
    The ordering is different though, but it suffices
    to swap $f_i$ with $f_{i + A.n}$ to make the ordering the same.

    Thus, we conclude that the two packages are the same.

    $\blacksquare$
\end{lemma}

\subsection{Syntactical Conventions for Packages}